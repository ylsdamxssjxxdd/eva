# 机体（EVA）软件技术说明书

本文面向研发与维护人员，基于当前仓库源码与 README，总结系统的整体架构、关键模块、数据/控制流、实现细节、扩展点与运维要点。文中术语以项目内命名为准（类/文件/宏等），并尽量对设计动机与权衡给出解释，便于后续演进与重构。

版本与环境假设
- 语言/框架：C++17 + Qt 5.15
- 平台：Windows、Linux（AppImage 打包可选），可选 CUDA/Vulkan/OpenCL 后端
- 推理内核：llama.cpp 的 tools/server 进程（本地模式）或 OpenAI 兼容接口（链接模式）
- 源码编码：UTF-8（无 BOM 推荐）

目录结构摘要
- 根：CMakeLists.txt；可生成 build/bin/eva(.exe)
- src/：核心代码（UI/网络/工具/后端管理等）
  - widget/：主窗口与 UI 逻辑
  - xnet.*：统一网络请求层（OpenAI 兼容，SSE 流）
  - xbackend.*：本地 llama-server 生命周期管理
  - xtool.*：工具（calculator/execute_command/knowledge/controller/...）
  - xmcp.* + mcp_*：MCP 客户端与工具聚合
  - utils/：设备发现/历史会话/自定义控件/系统检测等
  - prompt.*：提示词模板与工具协议
- thirdparty/：llama.cpp、whisper.cpp、stable-diffusion.cpp、QHotkey 等
- resource/：qrc 资源（图标、字体、样式）
- docs/：技术文档

----------------------------------------
## 一、系统总体架构

EVA 的目标是用“最轻”的桌面 UI 驱动“最通用”的 LLM 推理服务，具体采用“请求式统一 + 本地/远端二象性”的设计：
- 所有推理（对话/补完/多模态）都经由 xNet 发起 HTTP(S) 请求与 SSE 流解析；
- 本地模式下，仅把 llama.cpp server 作为“HTTP 服务”托管在本机，由 LocalServerManager 负责启动/重启/停止；
- 远端链接模式下，直接以 OpenAI 兼容协议对接外部端点（api_endpoint/key/model），xNet 透明复用；
- 工具调用采用“系统指令 + 输出解析 + 外部工具执行 + 二次发送”的闭环；
- 视觉/听觉/语音/知识库等增强能力由 Expend 窗口托管，保持主循环简洁与稳定。

核心组件关系（文本示意）：
- Widget(UI)
  - 发起装载/发送/设置/约定等操作
  - 维护历史会话（HistoryStore）
  - 与 xNet 双向通信（信号槽）
  - 与 LocalServerManager 交互（本地后端）
  - 与 xTool/xMcp/Expend 交互（工具与增强能力）
- xNet
  - 构造请求体（chat/completions）
  - 建立/解析 SSE 流，流式输出至 UI
  - 处理中止/超时/错误
- LocalServerManager
  - 按 SETTINGS 生成 llama-server 启动参数
  - 监听标准输出/错误以识别“ready”并转发日志
  - 负责重启/停止，保证端点可用
- xTool
  - 解析 <tool_call> JSON（在 <tool> 包裹内）
  - 执行内置工具或转发 MCP 工具调用
  - 将结果封装成“tool_response”返回 UI 以继续推理
- Expend
  - whisper 声转文、OuteTTS 文转声、Stable Diffusion/Flux 文生图
  - 知识库构建/嵌入服务管理与测试
  - 模型量化/转换、模型日志查看、MCP 服务配置等

线程模型（主流程）：
- main.cpp 在启动时将 gpuChecker/cpuChecker/xTool/xNet/xMcp 分别移动到独立 QThread；
- UI 线程仅做轻量渲染与信号派发，避免阻塞（命令执行、网络收发、TTS 进程等均在子线程）
- 所有跨线程通信通过 Qt 信号槽（QueuedConnection）完成，避免显式锁。

----------------------------------------
## 二、运行模式与状态机

模式（用户在装载时选择）：
- 本地模式（LOCAL_MODE）
  - 选择本地 gguf 模型路径
  - UI 调用 ensureLocalServer() 启动/重启本地 llama-server
  - xNet 指向 localhost:port 的 OpenAI 兼容端点
- 链接模式（LINK_MODE）
  - 填写 api_endpoint/api_key/api_model
  - 直接请求远端 /v1/chat/completions 或 /completions

状态（UI 行为约束）：
- 初始态：仅允许装载
- 装载中：禁用发送/设置；播放“转轮动画”；等待本地 server ready 或链接模式初始化完成
- 待机态：允许发送/设置/约定/重置；输出区只读或输入区只读取决于对话/补完
- 推理中：禁用装载/设置；显示转轮动画；支持用户中断
- 录音中：F2 开始/结束；结束后 whisper 解析并回填输入区

两个子状态：
- 对话状态（CHAT_STATE）：常规多轮消息（system+user/assistant），支持工具与多模态
- 补完状态（COMPLETE_STATE）：把输出区文本整体作为 prompt，模型补全后立即 reset（一次性）

----------------------------------------
## 三、后端管理（xbackend.*）

LocalServerManager 的职责是“按需让本地 llama.cpp server 处于可服务状态”：
- programPath()：统一经 utils/DeviceManager 解析当前后端（auto/cpu/cuda/vulkan/opencl）与可执行路径（llama-server[.exe]）。
- buildArgs()：根据 SETTINGS 生成参数：
  - -m <model>、--host、--port、-c <n_ctx>、-ngl（GPU 下才有效）、--threads、-b、--parallel 等；
  - --jinja 启用 toolcalling 模板；--reasoning-format none 表示把 <think> 内容混入文本；
  - LoRA：--no-mmap + --lora <path>；mmproj：--mmproj；
  - Flash-Attn、mlock/mmap 开关；
  - slots：--slots + --slot-save-path EVA_TEMP/slots（启用 KV 缓存保存/恢复端点）。
- ensureRunning()/restart()/stop()：
  - 通过 needsRestart() 判断当前参数与上一次是否不同
  - 启动后监听 stdout/stderr，遇到“server is listening on”/“listening at/on”判定 ready，并向 UI 发 serverReady(endpoint)
  - Windows 下 stop() 优先 terminate，不成则 taskkill /T /F（杀进程树）；Linux 直接 kill

端口/绑定策略（Widget::ensureLocalServer）：
- 用户端口为空：仅绑定 127.0.0.1，并随机选择可用端口（提升安全性）；
- 指定端口不可用：临时选择随机端口（UI 不修改）；
- 默认对外 0.0.0.0:port；采用 QTcpServer 探测是否可监听。

ngl 自动评估（一次性）：
- 若能获取可用显存 vfree，且（模型文件+mmproj）体积不超过 vfree 的 95%，首次尝试 ngl=999（全量 offload），待 server 日志给出 n_layer+1 后修正滑条显示；否则先走 CPU。

server 输出处理：
- 所有日志转发给增殖窗口“模型日志”页（便于查看加载与推理细节）。
- 速度/KV 相关解析详见 docs/kv_and_speed.md。

----------------------------------------
## 四、统一网络层（xnet.*）

xNet 将“本地/远端、对话/补完、多模态输入、工具停词”等统一为 OpenAI 兼容请求：
- createChatBody()：
  - 基本字段：model、stream=true、temperature（UI 0~1 映射至 OAI 0~2）、top_k、top_p、repeat_penalty、max_tokens
  - stop：来自 ENDPOINT_DATA.stopwords；
  - messages：调用 promptx::buildOaiChatMessages() 将 UI 历史数组正规化：
    - 确保首条 system 存在（无则注入）；
    - assistant 历史中的 <think>…</think> 被剥离，仅保留正文；
    - 多模态：text / image_url / input_audio 保留；历史中的 audio_url 旧格式在发送前转换为 input_audio{data,format}
  - id_slot：若 UI 保存了 llama-server 分配的会话 slot id，则复用，以命中 KV 缓存
- createCompleteBody()：
  - 旧式 /completions：prompt + n_predict + 采样参数等
- run()：
  - 构造 QNetworkRequest（Authorization、Accept:text/event-stream、HTTP2 允许、TransferTimeout 60s）
  - QNetworkAccessManager::post；连接 readyRead/finished/error/sslErrors
  - SSE 解析：以空行 \n\n 分割 event；忽略注释/空行；data: 行截取 JSON 体，支持数组/对象；
    - chat 格式：解析 choices[0].delta.content 或 choices[].delta；累计 tokens_；
      - 若开启“思考”输出（<think>），通过 thinkFlag 统计 reasoningTokensTurn_
    - completion 格式：content 或 completion；同样流式输出
    - 若 JSON 包含 timings：缓存 prompt_n/prompt_ms/predicted_n/predicted_ms，供 UI 收尾显示
  - 早停：
    - UI 发起 stop()；
    - 或者 assistant 输出在非 think 区域包含 DEFAULT_OBSERVATION_STOPWORD（工具协定中的 </tool_call>），视作一轮结束
  - finished：停止超时定时器；计算 httpCode；发 net2ui_reasoning_tokens(reasoningTokensTurn_) 与 net2ui_pushover()

错误/健壮性：
- 超时：120s 无数据/未结束即 abortActiveReply()
- abort 时解除所有连接，安全 deleteLater；
- SSL 错误仅提示不忽略（可按需扩展）。

----------------------------------------
## 五、UI 主流程（widget/*）

入口（main.cpp）：
- 创建 Widget/Expend/xTool/xNet/xMcp；
- 加载 qss、字体；初始化系统托盘与全局热键（QHotkey：F1 截图、F2 录音、Ctrl+Enter 发送）；
- 将若干对象移入独立线程，连接信号槽（见源码）；
- 读取 EVA_TEMP/eva_config.ini，应用上次配置（模型/语音/SD/嵌入/MCP 等）；
- 若配置为本地模式且模型存在 -> ensureLocalServer()；链接模式则 set_api()。

装载（on_load_clicked）：
- 弹出模式选择（本地/链接）
  - 本地：选择 gguf，设置 ui_SETTINGS.modelpath，清空 lora/mmproj；ensureLocalServer()
  - 链接：弹 API 对话框，确认后 set_api()

装载前动作（preLoad）：
- 清空输出/状态，切待机->装载中，播放“加载”转轮动画并计时；
- 若是从配置恢复，打印配置路径；打印模型路径；
- serverReady 后由 onServerReady() 收尾（切端点、注入 system、KV/UI 初始化、停止动画，更新窗口标题/图标）。

发送（on_send_clicked → api_send_clicked_slove）：
- 统一组装 ENDPOINT_DATA：
  - 对话：将输入区文本 + 已选图片/音频封装到 messagesArray；在输出区插入“user/assistant”蓝色分隔行（昵称来自约定）；
  - 补完：把输出区全量文本作为 input_prompt
- 采样参数：temp/repeat/top_k/top_p/n_predict；stopwords：来自约定（工具时会注入 </tool_call> 等）
- currentSlotId_：若历史会话与当前端点匹配则复用其 slot id（命中服务端 KV）
- 切状态为“推理中”，开始转轮动画；发 ui2net_data + ui2net_push

推理结束（recv_pushover）：
- 链接模式：计算兜底生成速度（tokens/秒）并输出；
- 将本轮 assistant 的 <think>…</think> 与正文分离；仅正文写入 messagesArray；reasoning 作为单独字段写入 HistoryStore，便于后续恢复显示；
- 若配置了工具：尝试解析 <tool>…</tool> 中 JSON（XMLparser）；
  - name==answer/response：直接结束
  - 否则 emit ui2tool_exec；在工具执行期间不停止转轮动画
- 无工具或工具链结束：normal_finish_pushover() 收尾（停止动画、解锁、展示待显示图片、KV 统计收束）。

多模态输入：
- 图像：拖拽/右键上传/F1 截图；user 消息 content 变为数组 [ {type:text}, {type:image_url: {url}}... ]
- 音频：选择 wav/mp3/ogg/flac；临时以 audio_url base64 内嵌；xNet 在发送前转换为 OpenAI input_audio 结构

右键菜单/托盘：
- 右击输入区：常见问题模板 + 图像上传 + 历史会话管理（openHistoryManager）
- 托盘：显示窗口/扩展窗口/快捷键/退出

历史会话（utils/history_store.*）：
- 结构：EVA_TEMP/history/<sessionId>/{meta.json,messages.jsonl}
- meta：id/title/endpoint/model/system/n_ctx/slot_id/started_at
- messages.jsonl：一行一个 JSON（role/content/可选 reasoning）
- UI“历史管理器”：支持搜索、恢复、重命名、删除、清空；恢复时若端点一致则带回 slot_id 以命中 KV。

KV 与速度显示：
- 详见 docs/kv_and_speed.md（本地模式以 server 日志为准，链接模式按 xNet 兜底）。

----------------------------------------
## 六、约定与提示词（prompt.* + xconfig.h）

约定（EVA_DATES）：
- date_prompt/user_name/model_name/is_load_tool/extra_stop_words
- UI“约定”对话框：
  - 切换模板（默认/自定义1/自定义2/彩蛋等），可编辑系统指令与昵称；
  - 勾选工具后 create_extra_prompt() 会在系统指令后拼接“工具协议 + 工具清单 + 工程师信息”。

工具协议（prompt.h）：
- EXTRA_PROMPT_FORMAT：
  - 在 <tools>…</tools> 中列出可用工具（名字/描述/JSON-Schema 摘要）
  - 要求模型以 <tool_call>…</tool_call> 返回一条 JSON（{"name":"…","arguments":{…}}）
  - 提供 answer 示例；工程师信息（ENGINEER_INFO/ENGINEER_SYSTEM_INFO）描述运行环境/限制（在 EVA_WORK 下执行、仅当前目录等）

默认常量（xconfig.h）：
- 角色名/停词/<think> 标签，采样参数默认值，n_ctx、并发/批/线程等隐含参数，平台相关（SFX_NAME、DEFAULT_SHELL、DEFAULT_PYTHON）
- 端点常量：CHAT_ENDPOINT/COMPLETION_ENDPOINT

----------------------------------------
## 七、工具系统（xtool.* + xmcp.*）

调用链路：assistant 输出（含 <tool>…</tool>）→ UI 解析 JSON → emit ui2tool_exec(tools_call) → xTool::Exec → 执行并以“tool_response …”回填 → UI 自动继续发送。

内置工具：
- calculator：基于 tinyexpr 计算 expression
- execute_command：在 EVA_WORK 目录下执行命令（尊重 DEFAULT_SHELL；Windows 下 CMD /C；Linux 下 /bin/sh -c）
  - 设置 PATH；合并 stdout/stderr；以本地编码/UTF-8 转码显示
- knowledge：调用“嵌入服务”进行查询（/v1/embeddings），计算余弦相似度，返回 Top-N（可配置）匹配文本段；
  - 需先在 Expend 中上传/切分/嵌入；
- controller：跨平台鼠标/键盘控制
  - Windows：SendInput/VK_*；Linux：XTest；提供 left_down/left_up/right_down/right_up/move/keyboard/time_span
- read_file / write_file / edit_file：工程师链条常用文件操作（edit_file 支持 expected_replacements 校验）
- stablediffusion：将 prompt 下发至 Expend 的 sd 流程并在完毕后显示图片
- mcp_tools_list：请求 xmcp 列出 MCP 服务的可用工具
- 其他：answer（模型向用户终止返回）

MCP 工具（xmcp.* + mcp_tools.h）：
- 配置（JSON）由 Expend“增殖窗口”提供；支持两种后端：
  - SSE：url（http[s]://host:port/path），握手 ping 与 get_server_capabilities，get_tools；
  - stdio：command + args + env，适合本地 CLI 方式托管 MCP 服务；
- 工具名约定：service@tool；xMcp::callTool 会拆分并转发到相应 ClientWrapper；
- 调用结果序列化为 JSON 字符串返回 xTool → UI。

安全与边界：
- execute_command/文件写入具备破坏性，默认仅在 EVA_WORK 下执行，UI 需提示风险（系统指令已强调约束）。
- controller 需要模型具备视觉能力与精准定位，属于高风险工具，应由用户主动勾选挂载。

----------------------------------------
## 八、增强能力（Expend）

1) 声转文（whisper）
- UI 侧：F2 录音（QAudioRecorder），再次 F2 结束；保存 WAV 并重采样 16kHz。
- 调度：调用 whisper-cli（thirdparty/whisper.cpp 子项目）进行解码，输出文本/字幕；
- 结果：通过信号回填输入框；支持手动选择音频批量转换。

2) 文转声（TTS）
- 两类声源：
  - 系统语音（QTextToSpeech）：直接朗读 UI 的流式输出（按标点分段）；
  - OuteTTS：进程方式生成音频文件，QMediaPlayer 监视播放完成。
- 相关参数：Speech_Params；Expend 保存待播文本与待播音频队列，轮询消费。

3) 知识库
- 目标：将用户上传的文档切分为片段，调用 /v1/embeddings 获得向量，入库（内存 + 配置持久化）。
- 嵌入服务：
  - 可选择本地“嵌入模型路径”并启动 server（embedding 模式），或直接复用远端端点；
  - UI 可测试嵌入维度、相似检索；
- 查询：xTool.embedding_query_process 以 HTTP POST 请求 embeddings 接口，拿到 1024 维向量后，与 DB 逐段计算余弦相似度，返回 Top-N 片段与相似度。

4) 文生图（Stable Diffusion / Flux）
- 通过 Expend 调用 thirdparty 的 sd 可执行（或脚本），支持多个参数模板（default/sd1.5-anything-3/sdxl/flux 等），支持图生图；
- 完成后将图片路径通过 <ylsdamxssjxxdd:showdraw> 特殊标记回传，UI 块状展示。

5) 模型量化/转换
- 量化：选择待量化模型、重要性矩阵、量化类型（f32/f16/bf16/q8_0），调用量化程序，日志输出与错误捕获；
- 转换：HF → GGUF（convert_hf_to_gguf.py）；支持脚本与参数选择，输出路径自动推断。

6) 模型日志/信息
- 实时接收 llama-server stdout/stderr；提供词表/记忆矩阵可视化（见 expend_brain.cpp）。

----------------------------------------
## 九、设备/可执行解析（utils/DeviceManager）

- availableBackends()：在应用目录（或 AppImage 的 /usr/bin）扫描 cpu/cuda/vulkan/opencl 子目录下是否存在 llama-server 可执行，推导“可用后端列表”。
- userChoice/effectiveBackend：记忆“用户选择”，auto 时按优先级 cuda>vulkan>opencl>cpu 选择第一个可用项。
- programPath(name)：拼出实际可执行路径，例如 。
- UI 设置页：展示“推理设备”下拉；当选择 CPU 时禁用 ngl（GPU offload 层数）。

----------------------------------------
## 十、国际化与可用性细节

- 双语词典：src/utils/ui_language.json；所有 UI 文案通过  按语言选择。
- 字体：Linux 从资源加载 SimSun，Windows 使用系统 SimSun，统一抗锯齿；
- 输入/输出：QPlainTextEdit/自定义控件；输出区滚动与状态区变色规则详见 widget_output.cpp。
- 动画：
  - 等待转轮（decode_* 系列）：在状态区固定行显示 Unicode “点字转轮”，字体不支持时回退 ASCII；显示累计用时；
  - 装载动画（load_* 系列）：文本像素风连线 + 进度推进（更偏彩蛋）。
- 托盘：隐藏主窗体时以托盘驻留（点击恢复）。
- 退出：closeEvent 中弹出“无上限进度”对话框，后台优雅停止本地 server，结束后退出进程。

----------------------------------------
## 十一、配置与持久化

- QSettings 文件：
  - 记录：模型路径、端口、采样/隐藏参数、约定模板（含自定义）、工具勾选、语音/SD/Whisper/MCP/嵌入配置、shell/python 等
- 历史会话：见“历史会话”小节
- 槽保存：llama-server  指向 ，方便恢复。

----------------------------------------
## 十二、错误处理与边界场景

- 网络：SSL 错误提示；SSE 超时 120s 保护；OperationCanceledError 与用户 abort 区分；
- 提前终止：UI 发 stop；或工具停词（</tool_call>）在非 think 段出现；
- 端口冲突：自动回退随机端口（本次运行有效）；空端口视为仅绑定 localhost（安全默认）；
- 设备切换：切换推理设备/影响后端参数的设置项 → ensureLocalServer() 可能触发重启；仅采样参数变化 → 仅 reset 上下文；
- 多模态：超大图片/音频仅以 base64 内嵌，注意输入长度上限（MAX_INPUT，UI 侧有截断策略）。

----------------------------------------
## 十三、构建与打包

- 构建（Release）：
  
- 运行：
  - Linux：./build/bin/eva
  - Windows：build\bin\eva.exe
- 打包：加 （Windows：组件聚合至 bin；Linux：AppImage，需要 linuxdeploy 等）
- 运行依赖：Qt5.15；可选 CUDA/Vulkan SDK；确保 PATH 中可找到第三方可执行。

编码与跨平台注意
- 源码 UTF-8（无 BOM 推荐）；源码中中文注释不得出现问号；Windows 终端建议启用 UTF-8；
- 换行符 LF 优先；在 Windows 编辑时保持一致；
- 若编码异常：先查编辑器设置 → file/iconv 确认 → 统一转 UTF-8 后再改。

----------------------------------------
## 十四、扩展指南

1) 新增内置工具
- 在 prompt.h 增加 TOOLS_INFO（name/description/arguments JSON-Schema），并加入 create_extra_prompt() 的可用清单
- 在 xTool::Exec 中添加分支，解析参数并实现功能；
- 如需 UI 配置项，在“约定”页增加勾选与说明；
- 注意：工具应返回明确的文本或特殊标记（如 stablediffusion 的 <ylsdamxssjxxdd:showdraw>），便于 UI 显示。

2) 接入新的 MCP 服务
- 在 Expend “MCP 服务”页填入 JSON 配置（url 或 command+args+env）；
- 若需要 UI 内置某些常用服务，可在 docs 补充样例；
- 工具命名统一为 service@tool，便于路由。

3) 扩展多模态输入
- promptx::buildOaiChatMessages 已支持 text/image_url/input_audio；
- 新增类型时在 fixContentArray 增加转换逻辑，并在 UI 侧组装对应结构；
- 注意大文件传输与端点兼容性。

4) 丰富速度/KV 统计
- 目前以服务端日志为主，流式兜底为辅；
- 可考虑：
  - 直接调用 llama.cpp server 的统计端点（若将来提供）
  - 在 UI 显式区分：Prompt TPS、Gen TPS、KV 命中/淘汰

5) 增强稳定性
- 网络：自动重试策略（幂等场景）；指数退避；
- 端口选择：在“绑定 0.0.0.0”时提示风险，并提供“仅本机”开关；
- 资源清理：确保所有子进程在退出前停止（当前已在 UI/Expend 做到）。

----------------------------------------
## 十五、关键代码索引

- 本地后端管理：src/xbackend.h:1, src/xbackend.cpp:1
- 统一网络层：src/xnet.h:1, src/xnet.cpp:1
- 提示词与工具协议：src/prompt.h:1, src/prompt_builder.*
- 主窗口与流程：src/widget/widget.cpp:1, src/widget/widget_slots.cpp:1, src/widget/widget_output.cpp:1
- 设置/约定/装载：src/widget/widget_settings.cpp:1, src/widget/widget_date.cpp:1, src/widget/widget_load.cpp:1
- 动画：src/widget/widget_anim.cpp:1
- 链接模式：src/widget/widget_link.cpp:1, src/widget/widget_api.cpp:1
- 工具：src/xtool.h:1, src/xtool.cpp:1
- MCP：src/xmcp.h:1, src/xmcp.cpp:1, src/mcp_tools.h:1
- 增殖窗口：src/expend/expend.* 与子模块（tts/whisper/sd/knowledge/...）
- 历史会话：src/utils/history_store.*
- 设备选择：src/utils/devicemanager.*

----------------------------------------
## 十六、常见问答（运维视角）

Q: 本地模式下 UI 卡在“装载中”？
- 看“模型日志”页是否出现“listening on/at”；
- 检查模型路径/权限；若使用 LoRA 建议关闭 mmap；
- GPU 后端试试把 ngl 改小，或先 CPU 装载以验证；
- 端口被占用时 UI 会临时换随机端口，注意防火墙策略。

Q: 链接模式无响应？
- 核对 api_endpoint 是否指向 /v1 根；
- HTTP/HTTPS 证书是否受信；
- SSE 是否被代理/防火墙拦截；
- 模型名不匹配可能返回 404/400。

Q: 工具链反复运行不收敛？
- 检查停词 </tool_call> 是否在模型输出末尾（非 think 区域）；
- 为“工程师链”增加“answer”终止步骤；
- 调整系统指令中关于工具的策略/节奏描述。

Q: 记忆进度条与速度数值怎么看？
- 参见 docs/kv_and_speed.md；本地以服务器日志为准，远端用兜底算法。

----------------------------------------
## 十七、设计权衡与后续方向

- 统一请求层：移除了本地“直解码”通道（旧 xbot），全部通过 HTTP 请求 + SSE，极大简化了 UI/状态机与多模态/工具链的组合复杂度；
- 本地后端最小托管：只负责进程与端点切换，不做额外 RPC 或共享内存，利于跨平台一致性；
- 工具调用“弱耦合”：仅靠系统指令 + XML 包裹 JSON + 停词，避免定制 tokens；
- 可演进方向：
  - 更细粒度的 server 能力探测（/slots、/models、/health）与 UI 映射；
  - KV 缓存的可视化与导出/导入；
  - 更丰富的多模态适配（视频帧、短语音段流式）；
  - 更严格的安全域与沙盒（工具链权限白名单）。

致谢
- llama.cpp、whisper.cpp、stable-diffusion.cpp 等上游项目；
- Qt 与 QHotkey；
- 所有参与者与用户反馈。

